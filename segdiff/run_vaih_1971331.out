## SLURM PROLOG ###############################################################
##    Job ID : 1971331
##  Job Name : run_vaih
##  Nodelist : gpu2006
##      CPUs : 4
##  Mem/Node : 16384 MB
## Directory : /oscar/home/xwang259/CSCI1430-Final-Project-MedImage-Segmentation/segdiff
##   Job Started : Thu May  9 04:42:07 PM EDT 2024
###############################################################################
2024-05-09-16-42-17-805267  Logging to /oscar/home/xwang259/CSCI1430-Final-Project-MedImage-Segmentation/logs/2024-05-09-16-42-17-760034_vaih_256_6_0.0001_4_100_0.1_0
2024-05-09-16-42-17-805345  {'data_dir': '', 'schedule_sampler': 'uniform', 'lr': 0.0001, 'weight_decay': 0.0, 'lr_anneal_steps': 0, 'clip_denoised': False, 'batch_size': 4, 'microbatch': -1, 'ema_rate': '0.9999', 'save_interval': 5000, 'start_print_iter': 75000, 'log_interval': 200, 'run_without_test': False, 'resume_checkpoint': '', 'use_fp16': True, 'fp16_scale_growth': 0.001, 'image_size': 256, 'num_channels': 128, 'num_res_blocks': 3, 'num_heads': 4, 'num_heads_upsample': -1, 'attention_resolutions': '16,8', 'dropout': 0.1, 'rrdb_blocks': 6, 'deeper_net': True, 'learn_sigma': False, 'sigma_small': False, 'class_cond': False, 'class_name': 'train', 'expansion': False, 'diffusion_steps': 100, 'noise_schedule': 'linear', 'timestep_respacing': '', 'use_kl': False, 'predict_xstart': False, 'rescale_timesteps': False, 'rescale_learned_sigmas': False, 'use_checkpoint': False, 'use_scale_shift_norm': False, 'seed': None}
2024-05-09-16-42-17-809713  log folder path: /oscar/home/xwang259/CSCI1430-Final-Project-MedImage-Segmentation/logs/2024-05-09-16-42-17-760034_vaih_256_6_0.0001_4_100_0.1_0
2024-05-09-16-42-17-858194  git commit hash 5b5e1261b10338c69ccb46ddebf49f6441adeb11
2024-05-09-16-42-17-858278  creating model and diffusion...
2024-05-09-16-42-19-102034  creating data loader...
2024-05-09-16-42-19-140263  gpu 0 / 4 val length 17
2024-05-09-16-42-19-140330  training...
2024-05-09-16-42-19-142736  model folder path
1
2
1
2
3
batch size: 4
microbatch: 4
  0%|          | 0/1 [00:00<?, ?it/s]3
batch size: 4
microbatch: 4
  0%|          | 0/1 [00:00<?, ?it/s]2024-05-09-16-42-21-402378  Distributed training requires CUDA. Gradients will not be synchronized properly!
0it [00:00, ?it/s]1
2
1
2
3
batch size: 4
microbatch: 4

  0%|          | 0/1 [00:00<?, ?it/s][A2024-05-09-16-42-21-550267  a
2024-05-09-16-42-21-550398  b
2024-05-09-16-42-21-550451  c
2024-05-09-16-42-21-550483  d
2024-05-09-16-42-21-550882  e
2024-05-09-16-42-21-550957  f
2024-05-09-16-42-21-550990  first
3
batch size: 4
microbatch: 4
  0%|          | 0/1 [00:00<?, ?it/s]--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec noticed that process rank 1 with PID 1470824 on node gpu2006 exited on signal 9 (Killed).
--------------------------------------------------------------------------
slurmstepd: error: Detected 1 oom_kill event in StepId=1971331.batch. Some of the step tasks have been OOM Killed.
